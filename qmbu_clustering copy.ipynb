{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from umap import UMAP\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kodu tutarlƒ± hale getir\n",
    "# resultstaki alanlarƒ± tespit et.\n",
    "# Bu alanlarƒ±n fotoƒüraflarƒ±nƒ± bul\n",
    "# results'ƒ± yorumla\n",
    "\n",
    "path = './'\n",
    "filename = 'joined_nufus.csv' \n",
    "\n",
    "df = pd.read_csv(path+filename)\n",
    "\n",
    "df['area'] = df['area']/1000000\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veride null olduƒüunu biliyoruz. Bu null veriler AFAD'ƒ±n mahalleyi yanlƒ±≈ü i≈üaretlemesinden kaynaklƒ±. \n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mahallelerdeki n√ºfus histogramƒ±\n",
    "sns.histplot(data=df[['mahalle_adi','mahalle_n√ºfus']].drop_duplicates(), x=\"mahalle_n√ºfus\", kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Burada mahalle bazƒ±nda bina mesafelerini stacked bar chart halinde √ßƒ±kartƒ±yoruz.\n",
    "#¬†Ancak n√ºfus bina sayƒ±sƒ±nƒ± pozitif y√∂nde etkileyecek. Normalize etmemiz lazƒ±m. \n",
    "df_building_counts = df[['ilce_adi','num_of_buildings_between_0_15','num_of_buildings_between_15_30','num_of_buildings_between_30_50']]\n",
    "df_building_counts = df_building_counts.rename(columns={\n",
    "    'num_of_buildings_between_0_15': '0_15m',\n",
    "    'num_of_buildings_between_15_30': '15_30m',\n",
    "    'num_of_buildings_between_30_50': '30_50m',\n",
    "    'ilce_adi':'District'\n",
    "})\n",
    "grouped = df_building_counts.groupby('District')[['0_15m', '15_30m', '30_50m']].sum()\n",
    "\n",
    "# Optional: sort by total building count (largest first)\n",
    "grouped['total'] = grouped.sum(axis=1)\n",
    "grouped = grouped.sort_values(by='total', ascending=False)\n",
    "grouped.drop(columns='total', inplace=True)\n",
    "\n",
    "# Plot stacked bar chart\n",
    "grouped.plot(\n",
    "    kind='bar',\n",
    "    stacked=True,\n",
    "    figsize=(14, 7),\n",
    "    colormap='viridis'\n",
    ")\n",
    "\n",
    "plt.title(\"Stacked Bar Chart of Building Counts by Proximity to EAZs by District\")\n",
    "plt.xlabel(\"District\")\n",
    "plt.ylabel(\"Number of Buildings\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.legend(title=\"Proximity to EAZ\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mahallelerdeki bina sayƒ±larƒ±nƒ±n n√ºfusa b√∂l√ºnm√º≈ü stacked bar chart'ƒ±\n",
    "# Bu bar chartƒ±n y axis'i bize √ßok bir ≈üey s√∂ylemiyor sadece bar chartlarƒ± birbirleriyle kar≈üƒ±la≈ütƒ±racaƒüƒ±z.\n",
    "# Aalar'ƒ± exclude ettik √ß√ºnk√º n√ºfusu d√º≈ü√ºk ve dar bir alan olduƒüu i√ßin veriyi √ßarpƒ±tƒ±yordu.\n",
    "df_building_counts = df[['ilce_adi','num_of_buildings_between_0_15','num_of_buildings_between_15_30','num_of_buildings_between_30_50','mahalle_n√ºfus']]\n",
    "df_building_counts = df_building_counts.rename(columns={\n",
    "    'num_of_buildings_between_0_15': '0_15m',\n",
    "    'num_of_buildings_between_15_30': '15_30m',\n",
    "    'num_of_buildings_between_30_50': '30_50m',\n",
    "    'mahalle_n√ºfus': 'neg_pop',\n",
    "    'ilce_adi':'District'\n",
    "})\n",
    "df_building_counts = df_building_counts[df_building_counts['District'] != 'ADALAR']\n",
    "\n",
    "grouped = df_building_counts.groupby('District')[['0_15m', '15_30m', '30_50m','neg_pop']].sum()\n",
    "\n",
    "grouped['normalized_15'] = grouped['0_15m'] / grouped['neg_pop']\n",
    "grouped['normalized_30'] = grouped['15_30m'] / grouped['neg_pop']\n",
    "grouped['normalized_50'] = grouped['30_50m'] / grouped['neg_pop']\n",
    "\n",
    "grouped_normalized = grouped[['normalized_15','normalized_30','normalized_50']]\n",
    "\n",
    "# Optional: sort by total building count (largest first)\n",
    "grouped_normalized['total'] = grouped_normalized.sum(axis=1)\n",
    "grouped_normalized = grouped_normalized.sort_values(by='total', ascending=False)\n",
    "grouped_normalized.drop(columns='total', inplace=True)\n",
    "# Plot stacked bar chart\n",
    "grouped_normalized.plot(\n",
    "    kind='bar',\n",
    "    stacked=True,\n",
    "    figsize=(14, 7),\n",
    "    colormap='viridis'\n",
    ")\n",
    "\n",
    "plt.title(\"Stacked Bar Chart of Building Counts by Proximity to EAZs by District\")\n",
    "plt.xlabel(\"District\")\n",
    "plt.ylabel(\"Number of Buildings\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.legend(title=\"Proximity to EAZ\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fid'yi dropladƒ±k ki model eƒüitirken kullanmayalƒ±m.\n",
    "df = df.drop(['fid'], axis='columns')\n",
    "df = df.dropna(axis='index', how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow_method(df, max_k=15,scaler=StandardScaler):\n",
    "    numeric_cols = [col for col in df.select_dtypes(include='number').columns if col != 'cluster']\n",
    "    X = df[numeric_cols]\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    inertia = []\n",
    "    k_values = range(1, max_k + 1)\n",
    "\n",
    "    for k in k_values:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(X_scaled)\n",
    "        inertia.append(kmeans.inertia_)\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(k_values, inertia, marker='o')\n",
    "    plt.xticks(k_values)\n",
    "    plt.xlabel('Number of Clusters (k)')\n",
    "    plt.ylabel('Inertia (WSS)')\n",
    "    plt.title(f'Elbow Method for Optimal k with {scaler}')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "elbow_method(df, max_k=15,scaler=StandardScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_and_plot(\n",
    "    df: pd.DataFrame,\n",
    "    cluster_num: int = 3,\n",
    "    plot: bool = True,\n",
    "    scaler=StandardScaler(),\n",
    "    reduction_method: str = \"pca\"  # Options: 'pca', 'umap', None\n",
    "):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Select numeric columns (excluding pre-existing cluster)\n",
    "    numeric_cols = [col for col in df.select_dtypes(include='number').columns if col != 'cluster']\n",
    "    X = df[numeric_cols]\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Fit KMeans\n",
    "    kmeans = KMeans(n_clusters=cluster_num, random_state=42)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    df['cluster'] = labels\n",
    "\n",
    "    # Evaluation\n",
    "    silhouette = silhouette_score(X_scaled, labels)\n",
    "    calinski = calinski_harabasz_score(X_scaled, labels)\n",
    "    davies = davies_bouldin_score(X_scaled, labels)\n",
    "\n",
    "    print(\"üìä Evaluation Results\")\n",
    "    print(f\"Silhouette Score:       {silhouette:.3f}\")\n",
    "    print(f\"Calinski-Harabasz Score: {calinski:.1f}\")\n",
    "    print(f\"Davies-Bouldin Score:    {davies:.2f}\")\n",
    "    print(\"\\nüî¢ Cluster Sizes:\")\n",
    "    print(df['cluster'].value_counts())\n",
    "\n",
    "    # Plotting\n",
    "    if plot:\n",
    "        if reduction_method == \"pca\":\n",
    "            X_plot = PCA(n_components=2).fit_transform(X_scaled)\n",
    "            x_label, y_label = \"PCA 1\", \"PCA 2\"\n",
    "        elif reduction_method == \"umap\":\n",
    "            X_plot = UMAP(n_components=2, random_state=42).fit_transform(X_scaled)\n",
    "            x_label, y_label = \"UMAP 1\", \"UMAP 2\"\n",
    "        else:\n",
    "            if X_scaled.shape[1] > 2:\n",
    "                print(\"‚ùó Cannot plot high-dimensional data without dimension reduction.\")\n",
    "                return df, kmeans, scaler, numeric_cols\n",
    "            X_plot = X_scaled\n",
    "            x_label, y_label = numeric_cols[0], numeric_cols[1]\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        colors = plt.cm.viridis(np.linspace(0, 1, cluster_num))\n",
    "        for cluster_id, color in zip(range(cluster_num), colors):\n",
    "            cluster_points = X_plot[labels == cluster_id]\n",
    "            plt.scatter(cluster_points[:, 0], cluster_points[:, 1],\n",
    "                        s=50, color=color, label=f\"Cluster {cluster_id}\")\n",
    "\n",
    "        plt.title(f\"K-Means Clustering (k={cluster_num}) - {reduction_method.upper() if reduction_method else 'No Reduction'}\")\n",
    "        plt.xlabel(x_label)\n",
    "        plt.ylabel(y_label)\n",
    "        plt.legend(title=\"Clusters\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    return df, kmeans, scaler, numeric_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clustered, kmeans, scaler, numeric_cols = cluster_and_plot(df,cluster_num=4,plot=True,scaler=StandardScaler(),reduction_method='umap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clustered[df_clustered['cluster'] == 3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', '{:.5f}'.format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inverse standardized\n",
    "centroids_unscaled = pd.DataFrame(\n",
    "    scaler.inverse_transform(kmeans.cluster_centers_),\n",
    "    columns=numeric_cols\n",
    ")\n",
    "centroids_unscaled.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clustered.groupby('cluster')[numeric_cols].median()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clustered.groupby('cluster')[numeric_cols].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_feature_distributions(df, features, cluster_col='cluster'):\n",
    "    num_features = len(features)\n",
    "    num_cols = 2\n",
    "    num_rows = (num_features + 1) // num_cols\n",
    "\n",
    "    plt.figure(figsize=(12, 4 * num_rows))\n",
    "    \n",
    "    for idx, feature in enumerate(features):\n",
    "        plt.subplot(num_rows, num_cols, idx + 1)\n",
    "        sns.boxplot(data=df, x=cluster_col, y=feature, palette='viridis')\n",
    "        plt.title(f\"{feature} by Cluster\")\n",
    "        plt.xlabel(\"Cluster\")\n",
    "        plt.ylabel(feature)\n",
    "        plt.grid(True, axis='y')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distributions(df_clustered, numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('cluster')[numeric_cols].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clustered.to_csv(path+'df_clustered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_qmbu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
